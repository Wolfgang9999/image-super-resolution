{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Super Resolution with Enhanced SRGAN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import tensorflow as tf\n",
    "from modules.esrgan import rrdb_net, discriminator_net\n",
    "from modules.lr_scheduler import MultiStepLR\n",
    "from modules.data import load_dataset\n",
    "from modules.losses import get_pixel_loss, get_content_loss\n",
    "from modules.losses import get_discriminator_loss, get_generator_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT = 'esrgan-tf2'\n",
    "\n",
    "INITIAL_LR_G = 1e-4\n",
    "INITIAL_LR_D = 1e-4\n",
    "LR_RATE = 0.5\n",
    "LR_STEPS = [50000, 100000, 200000, 300000]\n",
    "ADAM_BETA1_G = 0.9\n",
    "ADAM_BETA2_G = 0.99\n",
    "ADAM_BETA1_D = 0.9\n",
    "ADAM_BETA2_D = 0.99\n",
    "\n",
    "PIXEL_CRITERION = 'l1'\n",
    "FEATURE_CRITERION = 'l2'\n",
    "GAN_TYPE = 'ragan'\n",
    "WEIGHT_PIXEL = 1e-2\n",
    "WEIGHT_FEATURE = 1.0\n",
    "WEIGHT_GAN = 5e-3\n",
    "\n",
    "HR_HEIGHT = 128\n",
    "HR_WIDTH = 128\n",
    "SCALE = 4\n",
    "BATCH_SIZE = 1\n",
    "BUFFER_SIZE = 10240\n",
    "INPUT_SHAPE=(None, None, 3)\n",
    "\n",
    "NUM_ITER = 2\n",
    "SAVE_STEPS =  0\n",
    "\n",
    "PRETRAIN_PATH =  \"saved/checkpoints/psnr\"\n",
    "CHECK_POINT_PATH =  \"saved/checkpoints/esrgan\"\n",
    "Path(CHECK_POINT_PATH).mkdir(parents=True, exist_ok=True)\n",
    "SAVE_GAN_PATH = \"saved/models/esrgan.h5\"\n",
    "Path(SAVE_GAN_PATH).parent.mkdir(parents=True, exist_ok=True)\n",
    "SAVE_DISC_PATH = \"saved/models/disc_gan.h5\"\n",
    "Path(SAVE_DISC_PATH).parent.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainESR():\n",
    "\n",
    "    dataset = load_dataset(HR_HEIGHT, HR_WIDTH, SCALE)\n",
    "    dataset = dataset.repeat()\n",
    "    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "    generator = rrdb_net(input_shape=INPUT_SHAPE,scale_factor=SCALE)\n",
    "    discriminator = discriminator_net(input_shape=INPUT_SHAPE)\n",
    "\n",
    "    learning_rate_G = MultiStepLR(INITIAL_LR_G, LR_STEPS, LR_RATE)\n",
    "    learning_rate_D = MultiStepLR(INITIAL_LR_D, LR_STEPS, LR_RATE)\n",
    "    optimizer_G = tf.keras.optimizers.Adam(learning_rate= learning_rate_G,\n",
    "                                        beta_1= ADAM_BETA1_G,\n",
    "                                        beta_2= ADAM_BETA2_G\n",
    "                                        )\n",
    "    optimizer_D = tf.keras.optimizers.Adam(learning_rate= learning_rate_D,\n",
    "                                        beta_1= ADAM_BETA1_D,\n",
    "                                        beta_2= ADAM_BETA2_D\n",
    "                                        )\n",
    "\n",
    "    pixel_loss = get_pixel_loss(PIXEL_CRITERION)\n",
    "    feature_loss = get_content_loss(FEATURE_CRITERION)\n",
    "    generator_loss = get_generator_loss(GAN_TYPE)\n",
    "    discriminator_loss = get_discriminator_loss(GAN_TYPE)\n",
    "\n",
    "    checkpoint = tf.train.Checkpoint(step=tf.Variable(0, name='step'),\n",
    "                                     optimizer_G=optimizer_G,\n",
    "                                     optimizer_D=optimizer_D,\n",
    "                                     model=generator,\n",
    "                                     discriminator=discriminator)\n",
    "    manager = tf.train.CheckpointManager(checkpoint=checkpoint,\n",
    "                                         directory=CHECK_POINT_PATH,\n",
    "                                         max_to_keep=3)\n",
    "    if manager.latest_checkpoint:\n",
    "        checkpoint.restore(manager.latest_checkpoint)\n",
    "        print('[*] load ckpt from {} at step {}.'.format(\n",
    "            manager.latest_checkpoint, checkpoint.step.numpy()))\n",
    "    else:\n",
    "        if tf.train.latest_checkpoint(PRETRAIN_PATH):\n",
    "            checkpoint.restore(tf.train.latest_checkpoint(PRETRAIN_PATH))\n",
    "            checkpoint.step.assign(0)\n",
    "            print(\"[*] training from pretrain model {}.\".format(\n",
    "                    PRETRAIN_PATH ))\n",
    "        else:\n",
    "            print(\"[*] cannot find pretrain model {}.\".format(\n",
    "                PRETRAIN_PATH))\n",
    "    print(\"Starting\")\n",
    "\n",
    "    @tf.function\n",
    "    def train_step(lr, hr):\n",
    "        print(\"here in train_step\")\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_hr = generator(lr, training=True)\n",
    "            real_logits = discriminator(hr, training=True)\n",
    "            fake_logits = discriminator(generated_hr, training=True)\n",
    "            losses_G = {}\n",
    "            losses_D = {}\n",
    "            losses_G['pixel'] = WEIGHT_PIXEL * pixel_loss(hr, generated_hr)\n",
    "            losses_G['feature'] = WEIGHT_FEATURE * feature_loss(hr, generated_hr)\n",
    "            losses_G['gan'] = WEIGHT_GAN * generator_loss(real_logits, fake_logits)\n",
    "            losses_D['disc'] = discriminator_loss(real_logits, fake_logits)\n",
    "            total_loss_G = tf.add_n([l for l in losses_G.values()])\n",
    "            total_loss_D = tf.add_n([l for l in losses_D.values()])\n",
    "\n",
    "      \n",
    "        grads_G = tape.gradient(\n",
    "            total_loss_G, generator.trainable_variables)\n",
    "        grads_D = tape.gradient(\n",
    "            total_loss_D, discriminator.trainable_variables)\n",
    "        optimizer_G.apply_gradients(\n",
    "            zip(grads_G, generator.trainable_variables))\n",
    "        optimizer_D.apply_gradients(\n",
    "            zip(grads_D, discriminator.trainable_variables))\n",
    "\n",
    "        return total_loss_G, total_loss_D, losses_G, losses_D\n",
    "\n",
    "    \n",
    "    \n",
    "    remain_steps = max(NUM_ITER - checkpoint.step.numpy(), 0)\n",
    "    pbar = tqdm(total=remain_steps, ncols=50)\n",
    "    print(\"before loop\")\n",
    "    for lr, hr in dataset.take(remain_steps):\n",
    "        print(\"starting\")\n",
    "        checkpoint.step.assign_add(1)\n",
    "        steps = checkpoint.step.numpy()\n",
    "        total_loss_G, total_loss_D, losses_G, losses_D = train_step(lr, hr)\n",
    "\n",
    "        pbar.set_description(\"loss_G={:.4f}, loss_D={:.4f}, lr_G={:.1e}, lr_D={:.1e}\".format(\n",
    "            total_loss_G.numpy(), total_loss_D.numpy(),\n",
    "            optimizer_G.lr(steps).numpy(), optimizer_D.lr(steps).numpy()))\n",
    "        pbar.update(1)\n",
    "        # if steps % SAVE_STEPS == 0:\n",
    "        #     manager.save()\n",
    "        #     print(\"\\n[*] save ckpt file at {}\".format(manager.latest_checkpoint))\n",
    "\n",
    "\n",
    "    # generator.save(SAVE_GAN_PATH)\n",
    "    # discriminator.save(SAVE_DISC_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[*] cannot find pretrain model saved/checkpoints/psnr.\n",
      "Starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": []
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before loop\n",
      "starting\n",
      "here in train_step\n",
      "here in train_step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loss_G=7.2190, loss_D=0.7070, lr_G=1.0e-04, lr_D=1"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "trainESR()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
